---
aliases:
  - rlhf
tags:
  - LLM
  - RLHF
title: rlhf
---

# rlhf
During pre-trainning the model is trainned on large corpus of data, hence during the "Adapt and align model" phase the modelmight not be correctly performing for the given task. For instance by, refusing to give an approtiate answer during inference.

RLHF can be seen as an extra layer of tunning to a instruct model to align it with human response.

## Overview on Reinforcement learning

Model the interactions between an agent and an environment.
The feedback loop goes as follows: 

- The agent chooses a possible action to modify the board. 
- This new state is interpreted in light of the reward function. 
- The agent receives the reward information and the new environment state.

The agent's decision is based on the RL policy, the learning algorithm.


## Human feedback
In the case of RLHF the feedback is provided by a human. For instance, given an instruct LLM it can generated multiple completion given a prompt and then a human ranks them. To further feed this into a model, a ranking can be decomposed into pairwise entries.
For instance one associates a pair of completion as (1,0) if the first completion is good and other is not. This is done for all (3 2) combinations.

One can also think of the simpler exemple of checking is a answer is appropriated or not.

## Reward model
Human data is not so easy to get as scrapping the web, so another idea is to create a small dataset of human feedback responde and train a model on it, to use as a feedback machine.

Reward models expect: prompt-completion as input and a reward, scalar value, as output. To train we feed in:
- Prompt-Completion1 -> scalar1
- Prompt-Completion2 -> scalar2
- Minimize log($\sigma$(scalar1- scalar2))

In the original [paper](https://arxiv.org/pdf/2009.01325), one takes a pre-trained transformer and adds a linear head randomly initialized.

Then the loop is as follows For each element in the Prompt dataset:
1. Send it to the Instruct LLM to get a completion.
2. Combine prompt and completion and send it to the reward model to get a scalar
3. Send the prompt-completion and the reward to the RL algorithm, update the model.
4. Go back to 1. with the same prompt 

This loop continues until a certain number of epoch is reached or a certain treshhold of positive reward is met.

A problem that can occur is reward hacking. For instance for prompt of the form "This is product is..." the model might learn a be reward by using positive word, so a positive completion after a few iteration might be "the best product in the world, nothing compares to it". To avoid this we we can introduce a companion agent, another LLM. The loop goes in the same way but there is a step 1.5, where the completion of the model being updated is compared to the the a frozen version of the instruct model. This comparition is made throught the calculation of the KL divergence (shift penalty). This extra information is sent to the reward model.

Furthermore, one can see that this essencialy double the memory requirments for trainning, to mitigate this we can use PEFT for the RL procedure.
