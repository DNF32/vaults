---
aliases:
  - encoder
tags:
  - LLM
  - encoder
title: encoder
---

# encoder
# How Does BERT Process Input Text?

BERT (Bidirectional Encoder Representations from Transformers) is an **encoder-only model** designed for language understanding tasks. Unlike GPT, which is autoregressive, BERT processes the entire input sequence simultaneously using **bidirectional attention**. Here's how it works:

---

## 1. Tokenization of the Input

Before processing, the input text (e.g., "The cat sat on the mat") is **tokenized**:
- BERT uses a **WordPiece tokenizer** to break the text into subwords or words.
- Special tokens `[CLS]` and `[SEP]` are added to indicate the start and separation of sentences.

### Example
- **Input**: `"The cat sat on the mat"`
- **Tokens**: `["[CLS]", "The", "cat", "sat", "on", "the", "mat", "[SEP]"]`
- **Token IDs**: `[101, 1996, 4938, 4521, 2006, 1996, 4332, 102]`

These token IDs are fed into BERT.

---

## 2. Positional Encoding

Since BERT processes all tokens simultaneously, it needs to understand the **order** of tokens. 
- BERT adds **positional embeddings** to the token embeddings, encoding each token's position in the sequence.

### Example
For the token `"The"`, its representation is:
- **Token embedding**: Encodes the word "The."
- **Positional embedding**: Encodes that "The" is the 1st token.

---

## 3. Encoding the Input with Bidirectional Attention

BERT uses **bidirectional self-attention** to process the entire sequence at once. This allows it to attend to tokens both before and after a given token.

### Self-Attention Mechanism
- For each token, BERT computes the relevance of every other token in the sequence.
- Bidirectional attention enables BERT to fully understand the context, making it highly effective for tasks like question answering and named entity recognition.

### Example
- For the token `"sat"`, BERT considers the context from both:
  - Previous tokens: `[The, cat]`
  - Subsequent tokens: `[on, the, mat]`

---

## 4. Producing Hidden States

As the input passes through BERT’s layers, each token’s representation (hidden state) is refined by considering the context from all other tokens.

### Example
For the token `"cat"`:
- Its hidden state captures contextual information like the relationship between `"cat"` and `"sat"` or `"mat"`.

BERT produces **hidden states for every token** in the sequence at each layer. The final hidden states encode the input's meaning.

---

## 5. Special Tokens and Outputs

### `[CLS]` Token
- The `[CLS]` token's final hidden state is treated as a representation of the entire input sequence.
- It is often used for tasks like classification (e.g., sentiment analysis).

### `[SEP]` Token
- The `[SEP]` token marks the end of a sentence and helps BERT distinguish between different segments in tasks like sentence-pair classification.

---

## Why BERT Processes the Entire Input at Once

- **Bidirectional Attention**: Unlike GPT, which processes tokens sequentially, BERT looks at the entire input simultaneously.
- **Parallel Processing**: By using bidirectional self-attention, BERT builds a deep understanding of the relationships between all tokens in the sequence.

---

## Example in Action

Given the input:

> "The cat sat on the mat."

BERT will:
1. Tokenize it as `["[CLS]", "The", "cat", "sat", "on", "the", "mat", "[SEP]"]`.
2. Add positional embeddings to each token.
3. Process all tokens simultaneously with bidirectional self-attention.
4. Produce contextualized hidden states for each token.
5. Use the `[CLS]` token’s representation for tasks like classification or the token-level representations for tasks like entity recognition.

---

## Summary

BERT processes input text by:
1. Tokenizing and adding special tokens like `[CLS]` and `[SEP]`.
2. Using bidirectional self-attention to consider the full context of the sequence.
3. Producing hidden states that encode the meaning of each token in context.

This bidirectional design makes BERT highly effective for language understanding tasks.


## How to use the decoder

Once the decoder is trainned we can fine tune it to variety of different tasks. To do this we need to attach output specific layers. Then those are trainned on the specific task we are trying to solve. Maybe we can keep frozen the LLM weights.



