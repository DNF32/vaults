---
aliases:
  - decoder
tags:
  - LLM
  - decoder
title: decoder
---

# decoder

# How Does GPT Process the Full Prompt?

Despite GPT being a **decoder-only model**, it effectively processes the entire **prompt** (input text) by leveraging its autoregressive architecture. Here's a detailed explanation:

---

## 1. Tokenization of the Prompt

Before processing, the input prompt (e.g., "Once upon a time") is **tokenized**:
- The text is broken into smaller units called **tokens** (e.g., words, subwords, or characters).
- Each token is assigned a unique numerical ID based on the model's vocabulary.

### Example
- **Prompt**: `"Once upon a time"`
- **Tokens**: `["Once", "upon", "a", "time"]`
- **Token IDs**: `[189, 345, 21, 876]`

These token IDs are then fed into the model.

---

## 2. Encoding the Prompt with Self-Attention

GPT uses a **transformer decoder** architecture, where each layer processes the input tokens using **self-attention** and **feed-forward networks**.

### Self-Attention Mechanism
- GPT applies the self-attention mechanism to calculate the relevance of each token to every other token **before it** in the sequence.
- GPT’s attention is **causal**, meaning it only attends to tokens that appear earlier in the sequence, ensuring no future tokens are "seen."

### Example
- For the token `"time"`, the model attends to `[Once, upon, a]` but not to any future tokens (because they haven’t been generated yet).

This process builds a contextual understanding of the entire prompt up to each token.

---

## 3. Hidden States Represent the Prompt

As the prompt passes through GPT's layers, each token is transformed into a **hidden state**, a high-dimensional vector encoding its meaning in the context of the entire prompt up to that point.

### Example
- Hidden state of `"time"`: Represents its contextual meaning considering the earlier tokens `[Once, upon, a]`.

---

## 4. Using the Prompt to Predict the Next Token

Once the full prompt is processed, GPT predicts the next token in the sequence:
1. The final hidden state of the last token (`"time"`) is fed into a **softmax layer**, computing probabilities for all possible tokens in the vocabulary.
2. GPT selects the token with the highest probability (or samples from the probability distribution, depending on the decoding strategy).

### Example
- **Predicted next token**: `"story"`

---

## 5. Extending the Sequence

After generating the next token, GPT appends it to the input prompt and repeats the process:
1. The new sequence (`"Once upon a time story"`) is tokenized.
2. Self-attention and hidden state computations are performed on the extended sequence.
3. The model predicts the next token.

This process continues until:
- A special **end-of-sequence token** is generated.
- A maximum length is reached.
- A specific stopping criterion is met.

---

## Why GPT Can Process the Full Prompt

Although GPT processes the prompt sequentially in a causal manner:
- It uses **positional encodings** to determine the position of each token, maintaining order and context.
- The **self-attention mechanism** ensures that each token can attend to all prior tokens, enabling the model to understand the full prompt holistically.

---

## Example in Action

Given the prompt:

> "The quick brown fox jumps over the lazy dog, and then"

GPT will:
1. Process the entire sequence up to `"then"`.
2. Use the hidden states of `"then"` to predict the next token (e.g., `"it"`).
3. Append `"it"` to the sequence and repeat until the desired output is complete.

---

## Summary

GPT processes the full prompt by:
1. Tokenizing it.
2. Applying causal self-attention to build contextual representations.
3. Autoregressively predicting one token at a time.

While it generates tokens sequentially, its transformer architecture ensures that the full prompt context is effectively incorporated into each step.

